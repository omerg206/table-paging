{
 
      "analysis": {
          "analyzer": {
              "my_analyzer": {
                  "tokenizer": "my_tokenizer"
              }
          },
          "tokenizer": {
              "my_tokenizer": {
                  "type": "ngram",
                  "min_gram": 2,
                  "max_gram": 10,
                  "token_chars": [
                      "letter",
                      "digit",
                      "whitespace",
                      "symbol",
                      "punctuation"

                  ]
              }

          }
      },
      "max_ngram_diff": 50
}

